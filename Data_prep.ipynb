{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data_prep_submission.ipynb","provenance":[{"file_id":"1IzPbxNFQmmSYk9s14L4YjBfUgACn9mW2","timestamp":1609239498271},{"file_id":"1G7NKeneJNyRtcRxLVbbF9jYtRyuTOa-R","timestamp":1592749700622},{"file_id":"https://github.com/satyajitghana/TSAI-DeepVision-EVA4.0/blob/master/Utils/Colab_25GBRAM_GPU.ipynb","timestamp":1592043804148}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1b6b3f82256a473ea8b24272d059814b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_dd9e8c0e194f443496a188c9137f10a6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ae899230a6014a0eb829e1d649d6eca3","IPY_MODEL_7157d99c01934d019d255f69be78934a"]}},"dd9e8c0e194f443496a188c9137f10a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ae899230a6014a0eb829e1d649d6eca3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c75ac81e0a774da8a95f5b98c9cf4f0c","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":4942386,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4942386,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_914f8f961ece43c2ae4a3f3521be7626"}},"7157d99c01934d019d255f69be78934a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d23018e208624437801701eb80d7f947","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4942386/4942386 [03:19&lt;00:00, 24729.58it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e2fa54fb5f384c15be855dee20c74649"}},"c75ac81e0a774da8a95f5b98c9cf4f0c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"914f8f961ece43c2ae4a3f3521be7626":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d23018e208624437801701eb80d7f947":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e2fa54fb5f384c15be855dee20c74649":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mPKnmrJgqq32","executionInfo":{"status":"ok","timestamp":1609359481344,"user_tz":-330,"elapsed":26772,"user":{"displayName":"Akarshan Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip6PuyEltOM31pT05QmOxbiTmiOEqZwPlCgGZz-A=s64","userId":"15825100627593803722"}},"outputId":"286b958e-8e15-43b6-e7c1-32bff7b2b1f8"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0QJOq9G5qrCx","executionInfo":{"status":"ok","timestamp":1609359482106,"user_tz":-330,"elapsed":27526,"user":{"displayName":"Akarshan Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip6PuyEltOM31pT05QmOxbiTmiOEqZwPlCgGZz-A=s64","userId":"15825100627593803722"}},"outputId":"f3deb326-494b-4b44-c9f8-c136da9dbca7"},"source":["import os\r\n","os.chdir(\"/content/drive/My Drive/Classroom/projects/Mercari\")\r\n","!ls -l"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 6189307\n","-rw------- 1 root root        151 Nov 19 17:35 akarshan.1711@gmail.com_CS1.gdoc\n","-rw------- 1 root root        151 Dec 16 13:22 EDA+FE.gdoc\n","-rw------- 1 root root    2441752 Dec 20 16:29 EDA.ipynb\n","-rw------- 1 root root      14393 Dec 27 21:06 FE+prep+modelling.ipynb\n","-rw------- 1 root root      30163 Dec 29 18:34 HptBrnandImpute.v1.0.ipynb\n","-rw------- 1 root root      40352 Dec 30 20:02 HptTfidf.v1.0.ipynb\n","-rw------- 1 root root     927353 Dec 28 15:17 mercari_mainV2.ipynb\n","-rw------- 1 root root    7996136 Dec 30 19:54 price_log2.pickle\n","-rw------- 1 root root   20384538 Dec 29 15:57 price_log_BrandImput.pickle\n","-rw------- 1 root root  308669128 Dec 10  2019 test_stg2.tsv.zip\n","-rw------- 1 root root 1407107658 Dec 30 19:54 tfidf2.pickle\n","-rw------- 1 root root  610480534 Dec 29 15:57 tfidf_BrandImpute.pickle\n","-rw------- 1 root root 3641944242 Dec 29 17:10 tfidf.pickle\n","-rw------- 1 root root  337809843 Nov 11  2017 train.tsv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"faDikRZQqrDj","executionInfo":{"status":"ok","timestamp":1609359484761,"user_tz":-330,"elapsed":30175,"user":{"displayName":"Akarshan Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip6PuyEltOM31pT05QmOxbiTmiOEqZwPlCgGZz-A=s64","userId":"15825100627593803722"}},"outputId":"e7c9ea01-adba-407b-b769-53928e10aef0"},"source":["#importing modules/libraries\r\n","import pandas as pd\r\n","import numpy as np\r\n","import scipy\r\n","import seaborn as sns\r\n","import matplotlib.pyplot as plt\r\n","import gc\r\n","import sys\r\n","import os\r\n","import psutil\r\n","# from scipy.stats import randint as sp_randint\r\n","# from scipy.stats import uniform as sp_uniform\r\n","\r\n","\r\n","from tqdm.notebook import tqdm\r\n","# from collections import Counter\r\n","# from collections import defaultdict \r\n","import re\r\n","import random\r\n","# from random import sample\r\n","# from bs4 import BeautifulSoup\r\n","import pickle\r\n","import inspect\r\n","import time\r\n","\r\n","import sklearn\r\n","from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\r\n","from sklearn.model_selection import train_test_split\r\n","from sklearn.preprocessing import StandardScaler, LabelBinarizer\r\n","from sklearn.model_selection import RandomizedSearchCV \r\n","from sklearn.model_selection import GridSearchCV\r\n","from sklearn.metrics import mean_squared_error\r\n","import lightgbm as lgb\r\n","from sklearn.linear_model import Lasso,Ridge\r\n","\r\n","\r\n","import string\r\n","# import emoji\r\n","# from wordcloud import WordCloud \r\n","import nltk\r\n","nltk.download(\"stopwords\")\r\n","# nltk.download(\"brown\")\r\n","# nltk.download(\"names\")\r\n","# nltk.download('punkt')\r\n","nltk.download('wordnet')\r\n","# nltk.download('averaged_perceptron_tagger')\r\n","# nltk.download('universal_tagset')\r\n","# from nltk.tokenize import word_tokenize\r\n","from nltk.corpus import stopwords\r\n","from nltk.stem.wordnet import WordNetLemmatizer\r\n","# from nltk.stem.porter import PorterStemmer\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","import warnings\r\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2W3LxvsTqrFt"},"source":["# function to load train as well as Test data(3 times larger than train data)\r\n","def Data(clock,n_rows):\r\n","# using n_rows to get a subset of data to debug\r\n","    if int(n_rows) == -1:\r\n","        tr = pd.read_csv('train.tsv', sep='\\t')\r\n","        ts = pd.read_csv('test_stg2.tsv.zip',sep ='\\t')\r\n","    else:\r\n","        tr = pd.read_csv('train.tsv', sep='\\t',nrows = n_rows)\r\n","        ts = pd.read_csv('test_stg2.tsv.zip',sep ='\\t',nrows = n_rows)\r\n","\r\n","    \r\n","\r\n","    tr = tr.drop(['train_id'], axis =1)\r\n","    ts = ts.drop(['test_id'], axis =1)\r\n","\r\n","# dropping rows with invalid price\r\n","    tr = tr.drop(tr[tr.price < 1.0].index)\r\n","    tr.reset_index(inplace=True)\r\n","# changing price to Normal distribution by log transformation so that linear\r\n","# models don't give negative prediction\r\n","    y = np.log1p(tr.price)\r\n","    df=pd.concat([tr,ts],axis=0, ignore_index=True)\r\n","    df['item_condition_id'] = df['item_condition_id'].astype('category')\r\n","    df['shipping'] = df['shipping'].astype('category')\r\n","\r\n","\r\n","    gc.collect()\r\n","   # print fucntion completion time and with function name\r\n","    print(f'[{round((time.time() - clock),2)}] {inspect.stack()[0][3]} completed')\r\n","    return df, y,round(tr.shape[0]*0.8),tr.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MtTie9hYqrG3"},"source":["def Impute(df, tr_len,clock):\r\n","  # imputing with a 'abs' or absent \r\n","    df['category_name'].fillna(value='abs', inplace=True)\r\n","    df['name'].fillna(value='abs', inplace=True)\r\n","    df['item_description'].fillna(value='No Description Yet', inplace=True)\r\n","\r\n","    # using brands from train data only\r\n","    tr = df.iloc[:tr_len,:]\r\n","    test = df.iloc[tr_len:,:]\r\n","\r\n","  # imputing with a 'abs' or absent for train data and using  brand\r\n","  # names form train data only as target and freqency encoding done \r\n","  # in later sections ofnotebook so avoiding data leakage\r\n","    tr['brand_name'].fillna(value='abs',inplace=True)\r\n","    brand_name =tr.brand_name.unique()\r\n","    test.loc[~test['brand_name'].isin(brand_name),'brand_name'] = 'abs'  \r\n","\r\n","   # print fucntion completion time and with function name\r\n","    print(f'[{round((time.time() - clock),2)}] {inspect.stack()[0][3]} completed')\r\n","    del brand_name\r\n","    del df\r\n","    gc.collect()\r\n","    return pd.concat([tr,test],axis=0, ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pi3LCsb_qrIE"},"source":["# a category column contains Nan or 3 or more sub category in it upto 5\r\n","# as rows with more than 3 or less than 3 categories are less than 0.1 percent, \r\n","# we make only 3 new cols with segregated category names \r\n","def sub_cat(row):\r\n","      try:\r\n","        split = row.split('/')\r\n","        if len(split) >= 3:\r\n","          return split[0],split[1],split[2]\r\n","        if len(split) == 2:\r\n","          return split[0], split[1], 'abs'\r\n","        elif len(split) == 1:\r\n","          return split[0], 'abs', 'abs'\r\n","        else:\r\n","          return 'abs', 'abs', 'abs'\r\n","      except Exception:\r\n","          return  'abs', 'abs', 'abs'\r\n","\r\n","# extracting extra features from data\r\n","def Extract_features(df,tr_len,clock):\r\n","  # regex used later in this section to count number of them in a text column\r\n","    RE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])\r\n","    non_alphanumpunct = re.compile(u'[^A-Za-z0-9\\.?!,; \\(\\)\\[\\]\\'\\\"\\$]+')\r\n","    \r\n","  # extracting sub categories\r\n","    print(f'[{round(time.time()-clock)}]Extracting Subcat')\r\n","    df['sc1'], df['sc2'],df['sc3'] =  zip(*df['category_name'].apply(sub_cat))\r\n","    df.drop(columns='category_name',inplace = True)\r\n","    df['sc1'] = df['sc1'].astype('category')\r\n","    df['sc2'] = df['sc2'].astype('category')\r\n","    df['sc3'] = df['sc3'].astype('category')\r\n","\r\n","  # has description or not/ missing value added as a feature\r\n","    print(f'[{round(time.time()-clock)}]Extracting HasDescription ')\r\n","    df['HasDescription'] = 1\r\n","    df.loc[df['item_description']=='No description yet', 'HasDescription'] = 0\r\n","    df['HasDescription'] =df['HasDescription'].astype('category')\r\n","\r\n","  # has price or not/ [rm] values in textual columns are indicative of presence \r\n","  # price in the data which has been cleaned as suggested by the compition itself\r\n","    print(f'[{round(time.time()-clock)}]Extracting HasPrice ')\r\n","    df['HasPrice'] = 0\r\n","    df.loc[df['item_description'].str.contains('[rm]', regex=False), 'HasPrice'] = 1\r\n","    df.loc[df['name'].str.contains('[rm]', regex=False), 'HasPrice'] = 1\r\n","    df['HasPrice'] =df['HasPrice'].astype('category')\r\n","\r\n","    gc.collect()\r\n","  # counting number of tokens in textual columns\r\n","    print(f'[{round(time.time()-clock)}]Extracting Token Count ')\r\n","    df['NameTokenCount'] = df['name'].str.split().apply(len)\r\n","    df['DescTokenCount'] = df['item_description'].str.split().apply(len)\r\n","    df['NameTokenCount'] = df['NameTokenCount'].astype('uint32')\r\n","    df['DescTokenCount'] = df['DescTokenCount'].astype('uint32')\r\n","\r\n","  # ratio of token token counts in name and description columns(2 textual cols)\r\n","\r\n","    print(f'[{round(time.time()-clock)}]Extracting Name to Desc token Ratio ')\r\n","    df['NameDescTokenRatio'] = df['NameTokenCount']/df['DescTokenCount']\r\n","    df['NameDescTokenRatio'] =df['NameDescTokenRatio'].astype('float32')\r\n","\r\n","  # adding missing value as a feature for brand\r\n","    print(f'[{round(time.time()-clock)}]Extracting HasBrand ')\r\n","    df['HasBrand'] =1\r\n","    df.loc[df['brand_name'] == 'abs', 'HasBrand'] = 0\r\n","    df['HasBrand'] =df['HasBrand'].astype('category')\r\n","\r\n","  # counting uppper and lower count of characters as EDA suggested phoney/\r\n","  # counterfiet items when listed uses too many bold and Caps charactes with emojis\r\n","    print(f'[{round(time.time()-clock)}]Extracting Lower count ')\r\n","    df['NameLowerCount'] = df.name.str.count('[a-z]')\r\n","    df['DescriptionLowerCount'] = df.item_description.str.count('[a-z]')\r\n","    df['NameLowerCount'] =df['NameLowerCount'].astype('uint32')\r\n","    df['DescriptionLowerCount'] =df['DescriptionLowerCount'].astype('uint32')\r\n","\r\n","\r\n","    print(f'[{round(time.time()-clock)}]Extracting Upper count ')\r\n","    df['NameUpperCount'] = df.name.str.count('[A-Z]')\r\n","    df['DescriptionUpperCount'] = df.item_description.str.count('[A-Z]')\r\n","    df['NameUpperCount'] =df['NameUpperCount'].astype('uint32')\r\n","    df['DescriptionUpperCount'] =df['DescriptionUpperCount'].astype('uint32')\r\n","\r\n","  # punctuation count\r\n","    print(f'[{round(time.time()-clock)}]Extracting Punctuation Count ')\r\n","    df['NamePunctCount'] = df.name.str.count(RE_PUNCTUATION)\r\n","    df['DescriptionPunctCount'] = df.item_description.str.count(RE_PUNCTUATION)\r\n","    df['NamePunctCount'] =df['NamePunctCount'].astype('uint32')\r\n","    df['DescriptionPunctCount'] =df['DescriptionPunctCount'].astype('uint32')\r\n","\r\n","  # punct count ratio\r\n","    print(f'[{round(time.time()-clock)}]Extracting Punctuation Ratio ')    \r\n","    df['NamePunctCountRatio'] = df['NamePunctCount'] / df['NameTokenCount']\r\n","    df['DescriptionPunctCountRatio'] = df['DescriptionPunctCount'] / df['DescTokenCount']\r\n","    df['NamePunctCountRatio'] =df['NamePunctCountRatio'].astype('float32')\r\n","    df['DescriptionPunctCountRatio'] =df['DescriptionPunctCountRatio'].astype('float32')\r\n","\r\n","  # digit count( if model can get a sense of bundled items)\r\n","    print(f'[{round(time.time()-clock)}]Extracting Digit count ')\r\n","    df['NameDigitCount'] = df.name.str.count('[0-9]')\r\n","    df['DescriptionDigitCount'] = df.item_description.str.count('[0-9]')\r\n","    df['NameDigitCount'] =df['NameDigitCount'].astype('uint32')\r\n","    df['DescriptionDigitCount'] =df['DescriptionDigitCount'].astype('uint32')\r\n","\r\n","  # emoji and/or other nonalphanum count\r\n","    print(f'[{round(time.time()-clock)}]Extracting NonAlphaNum count ')\r\n","    df['NonAlphaDescCount'] = df['item_description'].str.count(non_alphanumpunct)\r\n","    df['NonAlphaNameCount'] = df['name'].str.count(non_alphanumpunct)\r\n","    df['NonAlphaDescCount'] =df['NonAlphaDescCount'].astype('uint32')\r\n","    df['NonAlphaNameCount'] =df['NonAlphaNameCount'].astype('uint32')\r\n","\r\n","\r\n","    cols = set(df.columns.values)\r\n","    non_num_col = {'name', 'item_condition_id', 'brand_name',\r\n","                  'shipping', 'item_description', 'sc1',\r\n","                  'sc2', 'sc3','HasDescription','HasPrice','HasBrand',\r\n","                   'price','index'\r\n","                  }\r\n","\r\n","    cols_to_normalize = cols - non_num_col \r\n","   \r\n","  # normalizing all the counts and ratios\r\n","    print(f'[{round(time.time()-clock)}]Normalizing')\r\n","    df_to_normalize = df[list(cols_to_normalize)]\r\n","    df_to_normalize = (df_to_normalize - df_to_normalize.min()) / (df_to_normalize.max() - df_to_normalize.min())\r\n","\r\n","\r\n","    df = df[list(non_num_col)]\r\n","    df = pd.concat([df, df_to_normalize],axis=1)\r\n","\r\n","    df.drop(columns='index',inplace=True)\r\n","\r\n","    del(df_to_normalize)\r\n","    gc.collect()\r\n","\r\n","\r\n"," '''  extracting mean categorical and brand price with minding data leakage with addition of\r\n","  random noise so making data more robust. An idea taken form a some youtube video of a \r\n","  kaggle grandmaster . This noise addition clearly has impacted the performace of model very positively. '''\r\n","    \r\n","    print(f'[{round(time.time()-clock)}]Extracting Mean price Categories')\r\n","    mean_dc = {}\r\n","    tr = df.iloc[:tr_len,:]\r\n","    ts = df.iloc[tr_len:,:]\r\n","    lst = ['sc1','sc2','sc3', 'brand_name']\r\n","\r\n","  #imputing values for nan with mean\r\n","    def boundary_case(hmap,key):\r\n","      try:\r\n","        return float(hmap[key])*np.random.normal(1,0.1)\r\n","      except:\r\n","      '''  when cases in test data are not \r\n","        present in train data mean_dict[feat] returns a nan to tackle that this part \r\n","        has been added( tho with normal usage it does not occur as this has been \r\n","        taken care of in  the imputation part itself , i had an experiment run which\r\n","        produced those cases so made this part as permanent only) '''\r\n","        hmap.mean()*np.random.normal(1,0.1)\r\n","\r\n","\r\n","    for feat in lst:\r\n","      ''' for every categorical column in the list above  finding the mean price of \r\n","      every category in it and adding that price in a column with a noise added to it\r\n","      *np.randon.normal(1,0.1)'''\r\n","        mean_dc[feat] = tr.groupby(feat)['price'].mean().astype(np.float32)\r\n","        mean_dc[feat] /= np.max(mean_dc[feat])#normalising dict\r\n","        tr['MeanPrice_'+feat] = tr[feat].apply(lambda x : boundary_case(mean_dc[feat],x)).astype(np.float32)\r\n","        tr['MeanPrice_'+feat].fillna( mean_dc[feat].mean(), inplace=True  )\r\n","       \r\n","    \r\n","        ts['MeanPrice_'+feat] = ts[feat].apply(lambda x : boundary_case(mean_dc[feat],x)).astype(np.float32)\r\n","        ts['MeanPrice_'+feat].fillna( mean_dc[feat].mean(), inplace=True  )\r\n","    \r\n","\r\n","    tr.drop(columns='price',inplace = True)\r\n","    ts.drop(columns='price',inplace = True)\r\n","\r\n","    print(f'[{round((time.time() - clock),2)}] {inspect.stack()[0][3]} completed')\r\n","\r\n","    del df,mean_dc\r\n","    gc.collect()\r\n","    return pd.concat([tr,ts],axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ISKXX4lOqrKK"},"source":["def Make_text_column(df,clock):\r\n","    '''As we saw in EDA that brands with NAN values can be imputed with names and item_desciption \r\n","    columns as there are brand names prevelent with more than 40 to 45 percent of chance. \r\n","    So instead of imputing so many brands (43 percent), just creating a new column by merging\r\n","    brands_name, name and item_description and making a text column and letting tfidf taking care of it.\r\n","    '''\r\n","\r\n","    df['text'] = df['name'].astype(str)+' '+df['brand_name'].str.strip().astype(str)+' '+df['item_description'].str.strip().astype(str)\r\n","    df = df.drop(columns=['item_description'])\r\n","\r\n","\r\n","    def decontracted(text):\r\n","    # tried many kinds of regex to clean the data but final result wasnt effected much wiht\r\n","    # this part so only doing necessary onces\r\n","        try:\r\n","            text = re.sub(u\"won't\", \"will not\", text)\r\n","            text = re.sub(u\"can\\'t\", \"can not\", text)\r\n","            text = re.sub(u\"n\\'t\", \" not\", text)\r\n","            text = re.sub(u\"\\'t\", \" not\", text)\r\n","            # separating digits for a sense of count if bundled items sold \r\n","            text = u\" \".join(re.split('(\\d+)',text) )\r\n","\r\n","        except:\r\n","            print('error')\r\n","        return text\r\n","\r\n","    def clean(df,col):\r\n","        non_alphanums = re.compile(u'[^A-Za-z0-9 ]+')\r\n","        wl = WordNetLemmatizer()\r\n","        \r\n","        preprocessed_text = []\r\n","        for _,sentance in tqdm(df[col].iteritems(),total=df.shape[0]):\r\n","\r\n","            sentance = decontracted(sentance)\r\n","            \r\n","            # nonalphanumeric character removal\r\n","            sentance = non_alphanums.sub(u' ', sentance)\r\n","            \r\n","           ''' did not lemmatize cause takes a lot of time and has negligible to no \r\n","            effect on performance\r\n","            did not convert to lower case as this takes a lot of time and can be done\r\n","            interensicly within TFIDF and Count vectorization along with text standardization'''\r\n","            # lemmetizing\r\n","            # sentance = ' '.join(wl.lemmatize(word.strip()) for word in sentance.split())\r\n","            sentance = ' '.join(word.strip() for word in sentance.split())\r\n","\r\n","            preprocessed_text.append(sentance)\r\n","        df[col] = pd.Series(preprocessed_text).values  \r\n","        del preprocessed_text\r\n","        return df\r\n","\r\n","    \r\n","    print('Cleaning text')\r\n","    df= clean(df,'text')\r\n","    print(f'Done')\r\n","\r\n","    print(f'[{round((time.time() - clock),2)}] {inspect.stack()[0][3]} completed')\r\n","\r\n","\r\n","    gc.collect()\r\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jckn-IjqrN1"},"source":["''' As brand has close to 5000 categories, converting it to numbers \r\n"," by Frequency encoding brand with minding data leakage, Tho  lgbm handles high order\r\n"," categorical columns efficiently this encoding boosted performance instead of just\r\n"," using OHE, also it is with addition of random noise so making data more robust and \r\n"," enhancing performance I had chosen to do the same with sub categorical columns but the \r\n"," did not perform that well and took extra memory space too'''\r\n","def high_categorical(df,ts,col='brand_name'):\r\n","        dictionary_frq = df[col].value_counts().to_dict()\r\n","        dict_replace = {k:(v/max(dictionary_frq.values()) * np.random.normal(1,0.01)) for k,v in dictionary_frq.items()}\r\n","        \r\n","        col_cat: pd.Series.astype('float16') = df[col].map(dict_replace)\r\n","        col_cat_ts: pd.Series.astype('float16') = ts[col].map(dict_replace)\r\n","        del dictionary_frq\r\n","        del dict_replace\r\n","        gc.collect()\r\n","        return col_cat.values.reshape(-1,1),col_cat_ts.values.reshape(-1,1)\r\n","\r\n","'''Converting all the data till yet to numeric form if not yet done'''\r\n","def Convert_to_predictor(df, tr_len,clock,stopwords=stopwords,high_categorical=high_categorical):\r\n","    try:\r\n","      df.drop(columns='index',inplace = True)\r\n","    except:\r\n","      pass\r\n"," \r\n","    df_dummies = scipy.sparse.csc_matrix(pd.get_dummies(df[['item_condition_id', 'shipping',\\\r\n","                                                            'HasDescription', 'HasPrice','HasBrand',\\\r\n","                                                            'sc1','sc2','sc3']], sparse=True).values)\r\n","    \r\n","    df.drop(columns=['item_condition_id', 'shipping','HasDescription', 'HasPrice','HasBrand'],inplace=True)\r\n","    df.drop(columns=['sc1','sc2','sc3'],inplace=True)\r\n","\r\n","    print(f'[{round((time.time() - clock),2)}]Transform categories data completed.')\r\n","\r\n","\r\n","    cols = ['NamePunctCount', 'NameDigitCount', 'DescriptionDigitCount',\\\r\n","            'NameUpperCount', 'DescriptionPunctCount', 'DescriptionPunctCountRatio', \\\r\n","            'DescTokenCount', 'DescriptionUpperCount', 'NonAlphaDescCount', \\\r\n","            'NonAlphaNameCount', 'NameTokenCount', 'NameLowerCount', \\\r\n","            'NameDescTokenRatio', 'DescriptionLowerCount', 'NamePunctCountRatio',\\\r\n","            'MeanPrice_sc1','MeanPrice_sc2','MeanPrice_sc3','MeanPrice_brand_name']\r\n","   \r\n","    df_num = scipy.sparse.csc_matrix(df[cols].values)\r\n","    \r\n","    df.drop(columns=cols,inplace=True)\r\n","   \r\n","    print(f'[{round((time.time() - clock),2)}]Transform numeric  data completed.')\r\n","\r\n","    gc.collect()\r\n","\r\n","    tr = df.iloc[:tr_len,:]\r\n","    test = df.iloc[tr_len:,:]\r\n","    gc.collect()\r\n","    del df \r\n","    vect = CountVectorizer(ngram_range=(1,3),min_df=5, max_df=0.85,\r\n","                         lowercase=True, max_features=50000,\r\n","                        analyzer='word', strip_accents = 'ascii', \r\n","                        stop_words= set(stopwords.words(\"english\")))\r\n","    \r\n","    tr_name = scipy.sparse.csr_matrix(vect.fit_transform(tr.name))\r\n","    ts_name = scipy.sparse.csr_matrix(vect.transform(test.name))\r\n","    df_name = scipy.sparse.vstack((tr_name,ts_name),format='csc')\r\n","\r\n","    tr.drop(columns=['name'],inplace=True)\r\n","    test.drop(columns=['name'],inplace=True)\r\n","  \r\n","    del vect,ts_name,tr_name\r\n","    \r\n","    print(f'[{round((time.time() - clock),2)}]Transform name data completed.')\r\n","\r\n","\r\n","\r\n","    vect = TfidfVectorizer(ngram_range=(1,3),min_df=5, max_df=0.85,\r\n","                         lowercase=True, max_features=100000,\r\n","                        analyzer='word', strip_accents = 'ascii', smooth_idf=True,stop_words= set(stopwords.words(\"english\")))\r\n","    \r\n","    tr_text = scipy.sparse.csr_matrix(vect.fit_transform(tr.text))\r\n","    ts_text = scipy.sparse.csr_matrix(vect.transform(test.text))\r\n","    df_text = scipy.sparse.vstack((tr_text,ts_text),format='csc')\r\n","\r\n","    tr.drop(columns=['text'],inplace=True)\r\n","    test.drop(columns=['text'],inplace=True)\r\n","  \r\n","    del vect,ts_text,tr_text\r\n","    \r\n","    print(f'[{round((time.time() - clock),2)}]Transform text data completed.')\r\n","\r\n","\r\n","  \r\n","\r\n","    # frequency encoding brands\r\n","    tr_brand,ts_brand = high_categorical(tr,test)\r\n","    tr_brand,ts_brand = scipy.sparse.csr_matrix(tr_brand),scipy.sparse.csr_matrix(ts_brand)\r\n","    df_brand = scipy.sparse.vstack((tr_brand,ts_brand),format='csc')\r\n","    \r\n","    tr.drop(columns=['brand_name'],inplace=True)\r\n","    test.drop(columns=['brand_name'],inplace=True)\r\n","    del tr_brand,ts_brand,high_categorical\r\n","\r\n","    print(f'[{round((time.time() - clock),2)}]Transform brand data completed.')\r\n","\r\n","\r\n","\r\n","    df_merge = scipy.sparse.hstack((df_brand,df_dummies, df_num,df_name, df_text ))\r\n","    print('Merge all data completed.')\r\n","\r\n","    \r\n","    del df_brand,df_dummies,df_num,df_text,df_name\r\n","    print(f'[{round((time.time() - clock),2)}] {inspect.stack()[0][3]} complete')\r\n","\r\n","    gc.collect()\r\n","    return df_merge"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7FdOPtcOrVQO"},"source":["# dummy function to work with just Train data\r\n","def Data_tronly(clock,n_rows):\r\n","  if int(n_rows) == -1:\r\n","    df = pd.read_csv('train.tsv', sep='\\t')\r\n","  else:\r\n","    df = pd.read_csv('train.tsv', sep='\\t',nrows =n_rows)\r\n","\r\n","  df = df.drop(['train_id'], axis =1)\r\n","  df = df.drop(df[df.price <= 1.0].index)\r\n","  df.reset_index(inplace=True)\r\n","  df['item_condition_id'] = df['item_condition_id'].astype('category')\r\n","  # df =df[df['brand_name'].notnull()]\r\n","  y = np.log1p(df.price)\r\n","  gc.collect()\r\n","  print(f'[{round((time.time() - clock),2)}] {inspect.stack()[0][3]} completed')\r\n","  return df, y,round(df.shape[0]*0.8),df.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3VVMJKvwrVNz","colab":{"base_uri":"https://localhost:8080/","height":542,"referenced_widgets":["1b6b3f82256a473ea8b24272d059814b","dd9e8c0e194f443496a188c9137f10a6","ae899230a6014a0eb829e1d649d6eca3","7157d99c01934d019d255f69be78934a","c75ac81e0a774da8a95f5b98c9cf4f0c","914f8f961ece43c2ae4a3f3521be7626","d23018e208624437801701eb80d7f947","e2fa54fb5f384c15be855dee20c74649"]},"executionInfo":{"status":"ok","timestamp":1609361271835,"user_tz":-330,"elapsed":1124519,"user":{"displayName":"Akarshan Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip6PuyEltOM31pT05QmOxbiTmiOEqZwPlCgGZz-A=s64","userId":"15825100627593803722"}},"outputId":"7f554727-3f00-478d-fdac-c87de0bfb454"},"source":["clock =time.time()\r\n","df,y, tr_len,whole_tr= Data(clock,n_rows = -1)\r\n","gc.collect()\r\n","df = Impute(df,tr_len,clock)\r\n","gc.collect()\r\n","df = Extract_features(df,tr_len,clock)\r\n","gc.collect()\r\n","df = Make_text_column(df,clock) \r\n","gc.collect()\r\n","df = Convert_to_predictor(df,tr_len,clock)\r\n","gc.collect()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[30.18] Data completed\n","[33.36] Impute completed\n","[35]Extracting Subcat\n","[56]Extracting HasDescription \n","[57]Extracting HasPrice \n","[62]Extracting Token Count \n","[114]Extracting Name to Desc token Ratio \n","[114]Extracting HasBrand \n","[114]Extracting Lower count \n","[180]Extracting Upper count \n","[203]Extracting Punctuation Count \n","[221]Extracting Punctuation Ratio \n","[221]Extracting Digit count \n","[239]Extracting NonAlphaNum count \n","[265]Normalizing\n","[269]Extracting Mean price Categories\n","[309.73] Extract_features completed\n","Cleaning text\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b6b3f82256a473ea8b24272d059814b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=4942386.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Done\n","[445.71] Make_text_column completed\n","[521.32]Transform categories data completed.\n","[524.88]Transform numeric  data completed.\n","[620.49]Transform name data completed.\n","[1119.04]Transform text data completed.\n","[1120.62]Transform brand data completed.\n","Merge all data completed.\n","[1121.35] Convert_to_predictor complete\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"YFyVZ5KQrVLP"},"source":["with open('tfidf.pickle','wb') as f:\r\n","  pickle.dump(df,f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LCLQ8tK5v4Iw"},"source":["with open('price_log.pickle','wb') as f:\r\n","  pickle.dump(y,f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uEVN24LS_MR1","executionInfo":{"status":"ok","timestamp":1609361375748,"user_tz":-330,"elapsed":1225473,"user":{"displayName":"Akarshan Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip6PuyEltOM31pT05QmOxbiTmiOEqZwPlCgGZz-A=s64","userId":"15825100627593803722"}},"outputId":"c01f47dd-30bc-450c-e5cd-6922c0c0b2e8"},"source":["df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4942386, 151063)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p1iyRyGx_Y6K","executionInfo":{"status":"ok","timestamp":1609361376511,"user_tz":-330,"elapsed":1225296,"user":{"displayName":"Akarshan Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip6PuyEltOM31pT05QmOxbiTmiOEqZwPlCgGZz-A=s64","userId":"15825100627593803722"}},"outputId":"44b8f464-7aaa-44f9-f0c9-73aea990f388"},"source":["np.isnan(df.data).sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"rc5ZzhGQ3GQX"},"source":["# i have tried numerous variations of this notebook presenting the one which worked best yet."]}]}